{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to load CSV files from a folder and add a 'question_type' column\n",
    "def load_csv_with_question_type(folder_path):\n",
    "    # Initialize an empty DataFrame to store all data\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    # Loop through all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file is a CSV file\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Get the question_type from the file name (text prior to the first underscore)\n",
    "            label_type = file_name\n",
    "\n",
    "            # Load the CSV file into a DataFrame\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['label_type'] = label_type\n",
    "\n",
    "            # Append the data to the all_data DataFrame\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "    return all_data\n",
    "\n",
    "dataset = 'annotated-mimic'\n",
    "# train_test = ['test', 'train']\n",
    "train_test = ['prompts']\n",
    "\n",
    "model_id_list = ['Llama-3.2-1B-Instruct', 'Llama-3.2-1B_all_1_8_16', 'Llama-3.2-3B-Instruct', 'Llama-3.2-3B_all_1_8_16'] \n",
    "\n",
    "# model_id_list = ['Meta-Llama-3.1-8B-Instruct',\n",
    "#                  'Meta-Llama-3.1-70B-Instruct', 'Meta-Llama-3.1-8B_all_1',\n",
    "#                  'Meta-Llama-3.1-8B_all_1-25000-hardest', \n",
    "#                  'Meta-Llama-3.1-8B_all_1-25000-nosupport',\n",
    "#                  'Meta-Llama-3.1-8B_bool_num_1-10000-hardest'] #, \"meta-llama/Meta-Llama-3.1-70B-Instruct\"]\n",
    "\n",
    "# temperature_list = [0, 0.3, 0.5, 0.7, 0.8, 1]\n",
    "temperature_list = [0]\n",
    "\n",
    "param_folder_list = []\n",
    "for tt in train_test:\n",
    "    for model_id in model_id_list: \n",
    "        for temp in temperature_list:\n",
    "            if temp == 1:\n",
    "                top_p_list = [0.5, 0.7, 0.9]\n",
    "            else:\n",
    "                top_p_list = [1]\n",
    "                \n",
    "            for top_p in top_p_list:\n",
    "                param_folder_list.append(\n",
    "                    {\n",
    "                        'train_test': tt,\n",
    "                        'model_id': model_id,\n",
    "                        'temperature': temp,\n",
    "                        'top_p': top_p\n",
    "                    }\n",
    "                )\n",
    "\n",
    "# model_id = 'Meta-Llama-3.1-8B_all_1-25000-hardest'\n",
    "# temperature_list = [0, 0.2, 0.5, 0.7, 0.8]\n",
    "# top_p_list = [0.5, 0.8, 0.9, 0.95]\n",
    "\n",
    "# param_folder_list = []\n",
    "\n",
    "# for temp in temperature_list:\n",
    "#     for top_p in top_p_list:\n",
    "#         param_folder_list.append({'temperature': temp,\n",
    "#                                     'top_p': top_p\n",
    "#                                     }\n",
    "#                                 )\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "full_df = None\n",
    "for params in param_folder_list:\n",
    "    folder_path = f\"./outputs/{dataset}/params/{params['model_id']}/{params['temperature']}_{params['top_p']}\"\n",
    "    print(folder_path)\n",
    "    \n",
    "    # Check if folder_path exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        continue  # Skip to the next iteration if the folder doesn't exist\n",
    "    \n",
    "    df = load_csv_with_question_type(folder_path)\n",
    "    df['model'] = params['model_id']\n",
    "    df['temperature'] = params['temperature']\n",
    "    df['top_p'] = params['top_p']\n",
    "    df['train_test'] = params['train_test']\n",
    "\n",
    "    if full_df is None:\n",
    "        full_df = df\n",
    "    else:\n",
    "        full_df = pd.concat([full_df, df], axis=0)\n",
    "    \n",
    "    print(full_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_answer_from_json(text, start_delimiter='```json', end_delimiter='```'):\n",
    "    if not isinstance(text, str):\n",
    "        # If text is not a string, return None\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        start_index = text.index(start_delimiter) + len(start_delimiter)\n",
    "        end_index = text.index(end_delimiter, start_index)\n",
    "        json_string = text[start_index:end_index].strip()\n",
    "\n",
    "        # Extract the answer value using regex\n",
    "        match = re.search(r'\"answer\"\\s*:\\s*(\".*?\"|\\d+(\\.\\d+)?)', json_string)\n",
    "        if match:\n",
    "            value = match.group(1)\n",
    "            # Remove quotes if the value is a string\n",
    "            if value.startswith('\"') and value.endswith('\"'):\n",
    "                value = value[1:-1]\n",
    "            return value\n",
    "        return None\n",
    "    except (ValueError, AttributeError) as e:\n",
    "        # Handle ValueError and any unexpected AttributeError \n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_info_from_json(df, json_column):\n",
    "    extracted_data = []\n",
    "    failure_list = []\n",
    "\n",
    "    regex_patterns = {\n",
    "        \"question\": r'\"question\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        \"type\": r'\"type\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        \"answer\": r'\"answer\"\\s*:\\s*\"([^\"]+)\"|:\\s*([\\d\\.]+)',\n",
    "        \"section\": r'\"section\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        \"source\": r'\"source\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        \"explanation\": r'\"explanation\"\\s*:\\s*\"([^\"]+)\"'\n",
    "    }\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        json_string = row[json_column]\n",
    "        extracted_row = {\"question\": None, \"type\": None, \"answer\": None, \"section\": None, \"source\": None, \"explanation\": None}\n",
    "        \n",
    "\n",
    "        # if extracted_row['model'].isna() or extracted_row['label'].isna() or extracted_row['question_label'].isna():\n",
    "        #     print(row)\n",
    "        #     raise Exception('stop')\n",
    "        \n",
    "        # Handle leading/trailing unwanted characters\n",
    "        try:\n",
    "            json_string = json_string.strip().replace('***', '').replace('```', '').replace('json', '')\n",
    "        except:\n",
    "            failure_list.append(idx)\n",
    "            extracted_row['model'] = row['model']\n",
    "            extracted_row['label'] = row['label']\n",
    "            extracted_row['question_type'] = row['question_type']\n",
    "            extracted_row['label_type'] = row['label_type']\n",
    "            extracted_row['temperature'] = row['temperature']\n",
    "            extracted_row['top_p'] = row['top_p']\n",
    "           \n",
    "            extracted_data.append(extracted_row)\n",
    "            continue\n",
    "\n",
    "        # Handle cases where the string contains multiple JSON objects\n",
    "        json_objects = re.findall(r'\\{.*?\\}', json_string)\n",
    "        if json_objects:\n",
    "            json_string = json_objects[0]\n",
    "\n",
    "        # Try to parse the JSON string\n",
    "        try:\n",
    "            data = json.loads(json_string)\n",
    "            if isinstance(data, list):\n",
    "                data = data[0]  # Take the first object from a list\n",
    "\n",
    "            # Extract the required fields\n",
    "            for key in extracted_row:\n",
    "                extracted_row[key] = data.get(key)\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            # Fall back to regex if JSON parsing fails\n",
    "            for key, pattern in regex_patterns.items():\n",
    "                match = re.search(pattern, json_string)\n",
    "                if match:\n",
    "                    extracted_row[key] = match.group(1) or match.group(2)\n",
    "\n",
    "        extracted_row['model'] = row['model']\n",
    "        extracted_row['label'] = row['label']\n",
    "        extracted_row['question_type'] = row['question_type']\n",
    "        # extracted_row['question_label'] = row['question_label']\n",
    "        extracted_row['label_type'] = row['label_type']\n",
    "        extracted_row['temperature'] = row['temperature']\n",
    "        extracted_row['top_p'] = row['top_p']\n",
    "        \n",
    "\n",
    "        # Add even if we don't have the 'answer'\n",
    "        if extracted_row['answer'] is None:\n",
    "            failure_list.append(idx)\n",
    "            extracted_data.append(extracted_row)\n",
    "        else:\n",
    "            extracted_data.append(extracted_row)\n",
    "\n",
    "    extracted_df = pd.DataFrame(extracted_data)\n",
    "    return extracted_df, failure_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df, info_failure_list  = extract_info_from_json(full_df, 'output')\n",
    "print(info_df.shape, len(info_failure_list))\n",
    "# print('na label')\n",
    "# display(info_df[info_df['label'].isna()])\n",
    "display(info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df['question_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def preprocess_values(y_true, y_pred):\n",
    "    # Define a helper function to handle numeric and string comparisons\n",
    "    def normalize(val):\n",
    "        if isinstance(val, str):\n",
    "            # If string, strip spaces and lower the case\n",
    "            return val.strip().lower()\n",
    "        elif isinstance(val, (int, float)):\n",
    "            # If numeric, convert to float for comparison (handles 188 == 188.0)\n",
    "            return float(val)\n",
    "        else:\n",
    "            return val\n",
    "    \n",
    "    y_true_normalized = [normalize(val) for val in y_true]\n",
    "    y_pred_normalized = [normalize(val) for val in y_pred]\n",
    "    \n",
    "    return np.array(y_true_normalized), np.array(y_pred_normalized)\n",
    "\n",
    "def bootstrap_metric(y_true, y_pred, metric_func, n=1000):\n",
    "    \"\"\"Performs bootstrapping to generate confidence intervals.\"\"\"\n",
    "    bootstrapped_scores = []\n",
    "    \n",
    "    # Resample n times\n",
    "    for i in range(n):\n",
    "        # Resample with replacement\n",
    "        y_true_resampled, y_pred_resampled = resample(y_true, y_pred)\n",
    "        # Calculate metric on resampled data\n",
    "        score = metric_func(y_true_resampled, y_pred_resampled)\n",
    "        bootstrapped_scores.append(score)\n",
    "    \n",
    "    # Compute the 95% confidence interval (2.5th and 97.5th percentiles)\n",
    "    lower_bound = np.percentile(bootstrapped_scores, 2.5)\n",
    "    upper_bound = np.percentile(bootstrapped_scores, 97.5)\n",
    "    return np.mean(bootstrapped_scores), lower_bound, upper_bound\n",
    "\n",
    "# List to store metrics for each model and data type\n",
    "metrics = []\n",
    "\n",
    "# Fill missing values\n",
    "info_df['label'].fillna('NA', inplace=True)\n",
    "info_df['answer'].fillna('NA', inplace=True)\n",
    "\n",
    "def convert_to_binary(y_true, y_pred):\n",
    "    return np.array([1 if yt == yp else 0 for yt, yp in zip(y_true, y_pred)])\n",
    "\n",
    "# Example: iterate over the groups\n",
    "for (question_type, model, temperature, top_p), group in info_df.groupby(['question_type', 'model', 'temperature', 'top_p']):\n",
    "    y_true = group['label']\n",
    "    y_pred = group['answer']\n",
    "    \n",
    "    # Preprocess y_true and y_pred\n",
    "    y_true, y_pred = preprocess_values(y_true, y_pred)\n",
    "    y_true_binary = convert_to_binary(y_true, y_true)  # Ground truth stays as is\n",
    "    y_pred_binary = convert_to_binary(y_true, y_pred)  # Prediction binary matches with true values\n",
    "    \n",
    "    # Perform bootstrapping for accuracy\n",
    "    accuracy_mean, accuracy_lower, accuracy_upper = bootstrap_metric(y_true_binary, y_pred_binary, accuracy_score)\n",
    "    \n",
    "    # Perform bootstrapping for balanced accuracy\n",
    "    balanced_acc_mean, balanced_acc_lower, balanced_acc_upper = bootstrap_metric(y_true_binary, y_pred_binary, balanced_accuracy_score)\n",
    "    \n",
    "    # Perform bootstrapping for F1 score (macro)\n",
    "    macro_f1_mean, macro_f1_lower, macro_f1_upper = bootstrap_metric(y_true_binary, y_pred_binary, lambda y_true, y_pred: f1_score(y_true, y_pred, average='macro'))\n",
    "    \n",
    "    # Perform bootstrapping for F1 score (micro)\n",
    "    micro_f1_mean, micro_f1_lower, micro_f1_upper = bootstrap_metric(y_true_binary, y_pred_binary, lambda y_true, y_pred: f1_score(y_true, y_pred, average='micro'))\n",
    "    \n",
    "    # Append the results to the list\n",
    "    metrics.append({\n",
    "        'model': model,\n",
    "        'temperature': temperature,\n",
    "        'top_p': top_p,\n",
    "        'question_type': question_type,\n",
    "        'accuracy_mean': accuracy_mean,\n",
    "        'accuracy_ci': (accuracy_lower, accuracy_upper),\n",
    "        'balanced_acc_mean': balanced_acc_mean,\n",
    "        'balanced_acc_ci': (balanced_acc_lower, balanced_acc_upper),\n",
    "        'macro_f1_mean': macro_f1_mean,\n",
    "        'macro_f1_ci': (macro_f1_lower, macro_f1_upper),\n",
    "        'micro_f1_mean': micro_f1_mean,\n",
    "        'micro_f1_ci': (micro_f1_lower, micro_f1_upper)\n",
    "    })\n",
    "\n",
    "# Convert the results into a DataFrame for display\n",
    "metrics_info_df = pd.DataFrame(metrics)\n",
    "# pd.set_option('display.max_rows', 400)\n",
    "# display(metrics_info_df.sort_values(by='model'))\n",
    "# pd.set_option('display.max_rows', 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "def preprocess_values(y_true, y_pred):\n",
    "    # Define a helper function to handle numeric and string comparisons\n",
    "    def normalize(val):\n",
    "        if isinstance(val, str):\n",
    "            # If string, strip spaces and lower the case\n",
    "            return val.strip().lower()\n",
    "        elif isinstance(val, (int, float)):\n",
    "            # If numeric, convert to float for comparison (handles 188 == 188.0)\n",
    "            return float(val)\n",
    "        else:\n",
    "            return val\n",
    "    \n",
    "    y_true_normalized = [normalize(val) for val in y_true]\n",
    "    y_pred_normalized = [normalize(val) for val in y_pred]\n",
    "    \n",
    "    return np.array(y_true_normalized), np.array(y_pred_normalized)\n",
    "\n",
    "def bootstrap_metric(y_true, y_pred, metric_func, n=1000):\n",
    "    \"\"\"Performs bootstrapping to generate confidence intervals.\"\"\"\n",
    "    bootstrapped_scores = []\n",
    "    \n",
    "    # Resample n times\n",
    "    for i in range(n):\n",
    "        # Resample with replacement\n",
    "        y_true_resampled, y_pred_resampled = resample(y_true, y_pred)\n",
    "        # Calculate metric on resampled data\n",
    "        score = metric_func(y_true_resampled, y_pred_resampled)\n",
    "        bootstrapped_scores.append(score)\n",
    "    \n",
    "    # Compute the 95% confidence interval (2.5th and 97.5th percentiles)\n",
    "    lower_bound = np.percentile(bootstrapped_scores, 2.5)\n",
    "    upper_bound = np.percentile(bootstrapped_scores, 97.5)\n",
    "    return np.mean(bootstrapped_scores), lower_bound, upper_bound\n",
    "\n",
    "# List to store metrics for each model and data type\n",
    "metrics = []\n",
    "\n",
    "# Fill missing values\n",
    "info_df['label'].fillna('NA', inplace=True)\n",
    "info_df['answer'].fillna('NA', inplace=True)\n",
    "\n",
    "def convert_to_binary(y_true, y_pred):\n",
    "    return np.array([1 if yt == yp else 0 for yt, yp in zip(y_true, y_pred)])\n",
    "\n",
    "# Example: iterate over the groups with a progress bar\n",
    "grouped = info_df.groupby(['question_type', 'model', 'temperature', 'top_p'])\n",
    "\n",
    "# Use tqdm to wrap the loop for progress tracking\n",
    "for (question_type, model, temperature, top_p), group in tqdm(grouped, desc=\"Processing groups\"):\n",
    "    y_true = group['label']\n",
    "    y_pred = group['answer']\n",
    "    \n",
    "    # Preprocess y_true and y_pred\n",
    "    y_true, y_pred = preprocess_values(y_true, y_pred)\n",
    "    y_true_binary = convert_to_binary(y_true, y_true)  # Ground truth stays as is\n",
    "    y_pred_binary = convert_to_binary(y_true, y_pred)  # Prediction binary matches with true values\n",
    "    \n",
    "    # Perform bootstrapping for accuracy\n",
    "    accuracy_mean, accuracy_lower, accuracy_upper = bootstrap_metric(y_true_binary, y_pred_binary, accuracy_score)\n",
    "    \n",
    "    # Perform bootstrapping for balanced accuracy\n",
    "    balanced_acc_mean, balanced_acc_lower, balanced_acc_upper = bootstrap_metric(y_true_binary, y_pred_binary, balanced_accuracy_score)\n",
    "    \n",
    "    # Perform bootstrapping for F1 score (macro)\n",
    "    macro_f1_mean, macro_f1_lower, macro_f1_upper = bootstrap_metric(y_true_binary, y_pred_binary, lambda y_true, y_pred: f1_score(y_true, y_pred, average='macro'))\n",
    "    \n",
    "    # Perform bootstrapping for F1 score (micro)\n",
    "    micro_f1_mean, micro_f1_lower, micro_f1_upper = bootstrap_metric(y_true_binary, y_pred_binary, lambda y_true, y_pred: f1_score(y_true, y_pred, average='micro'))\n",
    "    \n",
    "    # Append the results to the list\n",
    "    metrics.append({\n",
    "        'model': model,\n",
    "        'temperature': temperature,\n",
    "        'top_p': top_p,\n",
    "        'question_type': question_type,\n",
    "        'accuracy_mean': accuracy_mean,\n",
    "        'accuracy_ci': (accuracy_lower, accuracy_upper),\n",
    "        'balanced_acc_mean': balanced_acc_mean,\n",
    "        'balanced_acc_ci': (balanced_acc_lower, balanced_acc_upper),\n",
    "        'macro_f1_mean': macro_f1_mean,\n",
    "        'macro_f1_ci': (macro_f1_lower, macro_f1_upper),\n",
    "        'micro_f1_mean': micro_f1_mean,\n",
    "        'micro_f1_ci': (micro_f1_lower, micro_f1_upper)\n",
    "    })\n",
    "\n",
    "# Convert the results into a DataFrame for display\n",
    "metrics_info_df = pd.DataFrame(metrics)\n",
    "# pd.set_option('display.max_rows', 400)\n",
    "# display(metrics_info_df.sort_values(by='model'))\n",
    "# pd.set_option('display.max_rows', 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Assuming metrics_info_df already has 'accuracy_mean', 'accuracy_ci', and 'num_samples'\n",
    "\n",
    "# First, calculate the number of samples per model/question_type\n",
    "metrics_info_df['num_samples'] = info_df.groupby(['model', 'question_type']).size().reset_index(name='count')['count']\n",
    "\n",
    "# Define a helper function to format the mean and 95% confidence interval as percentages\n",
    "def format_mean_ci_percentage(row):\n",
    "    mean = row['accuracy_mean'] * 100  # Convert to percentage\n",
    "    lower_ci, upper_ci = [ci * 100 for ci in row['accuracy_ci']]  # Convert CI to percentage\n",
    "    return f\"{mean:.1f}% ({lower_ci:.1f}%, {upper_ci:.1f}%)\"\n",
    "\n",
    "# Apply the formatting function to create a new column for formatted mean (95% CI) as percentages\n",
    "metrics_info_df['accuracy_mean_ci'] = metrics_info_df.apply(format_mean_ci_percentage, axis=1)\n",
    "\n",
    "# Now create a pivot table that displays the mean (95% CI) for each model and question type\n",
    "pivot_table = metrics_info_df.pivot_table(\n",
    "    index='model',\n",
    "    columns='question_type',\n",
    "    values='accuracy_mean_ci',\n",
    "    aggfunc='first'  # We use 'first' because we already have the mean (95% CI) precomputed\n",
    ")\n",
    "\n",
    "# Perform bootstrapping to calculate the weighted average with CI\n",
    "def bootstrap_weighted_avg(model_df, n=1000):\n",
    "    \"\"\"Bootstrap the weighted average accuracy and calculate the 95% CI.\"\"\"\n",
    "    bootstrapped_means = []\n",
    "    \n",
    "    # Perform bootstrapping n times\n",
    "    for _ in range(n):\n",
    "        # Resample with replacement\n",
    "        resampled_df = resample(model_df)\n",
    "        # Calculate the weighted average of the resampled data\n",
    "        weighted_avg = np.average(resampled_df['accuracy_mean'], weights=resampled_df['num_samples'])\n",
    "        bootstrapped_means.append(weighted_avg)\n",
    "    \n",
    "    # Calculate the mean and 95% confidence interval\n",
    "    mean_avg = np.mean(bootstrapped_means) * 100  # Convert to percentage\n",
    "    lower_ci = np.percentile(bootstrapped_means, 2.5) * 100  # Convert CI to percentage\n",
    "    upper_ci = np.percentile(bootstrapped_means, 97.5) * 100  # Convert CI to percentage\n",
    "    return mean_avg, lower_ci, upper_ci\n",
    "\n",
    "# Calculate the weighted average accuracy and 95% CI for each model\n",
    "weighted_avgs_with_ci = []\n",
    "for model, group in metrics_info_df.groupby('model'):\n",
    "    mean_avg, lower_ci, upper_ci = bootstrap_weighted_avg(group)\n",
    "    weighted_avgs_with_ci.append({\n",
    "        'model': model,\n",
    "        'weighted_avg_mean': mean_avg,\n",
    "        'weighted_avg_ci': (lower_ci, upper_ci)\n",
    "    })\n",
    "\n",
    "# Convert the results to a DataFrame for easier manipulation\n",
    "weighted_avg_df = pd.DataFrame(weighted_avgs_with_ci)\n",
    "\n",
    "# Add the weighted average with 95% CI as percentages to the pivot table\n",
    "weighted_avg_df['weighted_avg_mean_ci'] = weighted_avg_df.apply(\n",
    "    lambda row: f\"{row['weighted_avg_mean']:.1f}% ({row['weighted_avg_ci'][0]:.1f}%, {row['weighted_avg_ci'][1]:.1f}%)\", axis=1\n",
    ")\n",
    "\n",
    "# Merge the pivot table with the weighted average CI\n",
    "pivot_table['Average'] = weighted_avg_df.set_index('model')['weighted_avg_mean_ci']\n",
    "\n",
    "# Convert the pivot table to a CSV format (with ',' separator)\n",
    "# csv_output = pivot_table.to_csv()\n",
    "\n",
    "# Print the CSV output so you can directly copy it and paste into Excel\n",
    "# print(csv_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 400)\n",
    "# display(metrics_info_df[['question_type', 'model', 'temperature', 'top_p', 'accuracy']].sort_values(by='question_label'))\n",
    "# pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 400)\n",
    "# # display(metrics_info_df.groupby(['model', 'temperature', 'top_p', 'train_test'])[['auc', 'accuracy', 'balanced_accuracy', 'macro_f1', 'micro_f1']].mean())\n",
    "# display(metrics_info_df.groupby(['model', 'temperature', 'top_p'])[['macro_f1', 'micro_f1']].mean())\n",
    "# pd.set_option('display.max_rows', 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flash",
   "language": "python",
   "name": "flash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
