{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "input_file=\"all\"\n",
    "min_difficulty=1\n",
    "file_name = f\"synth_{input_file}_{min_difficulty}_df\"\n",
    "\n",
    "df = pd.read_csv(f\"../data/synthetic-raw/{file_name}.csv\", compression='gzip')\n",
    "display(df['question_type'].value_counts())\n",
    "print(df.shape)\n",
    "\n",
    "min_count = 25000\n",
    "# new_df = df.groupby('question_type').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n",
    "new_df = df.groupby('question_type').apply(lambda x: x.nlargest(min_count, 'difficulty')).reset_index(drop=True)\n",
    "display(new_df['question_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = f\"\"\"You are a clinical research assistant helping to accurately answer questions from clinical notes. You answer with a single valid JSON object based on the patient note. \n",
    "All dates have been shifted to keep data de-identified, so they may be in the future. We care only about information captured in the note, for example when asking what is the highest lab a patient has, we mean the highest lab recorded in the note. \n",
    "If you cannot find information to answer the question asked of you in the note answer Not Available, unless the rest of the prompt recommends something different. \n",
    "\"\"\"\n",
    "\n",
    "def create_prompt(note, question, qtype):\n",
    "    if qtype in ['yes', 'na-bool']:\n",
    "        type_str = \"str - 'Yes/No'\"\n",
    "        answer_str = \"str - 'Yes' or 'No' if the question can be answered based on the note, if the question cannot be answered based on the content of the note answer 'NA'\"\n",
    "        json_example = f\"\"\"```json\n",
    "{{\n",
    "    \"question\" : \"Does the note state that the patient is breathing normally on room air?\",\n",
    "    \"type\": \"Yes/No\",\n",
    "    \"answer\": \"No\",\n",
    " }}```\"\"\"\n",
    "        # \"difficulty\": \"2\",\n",
    "    elif qtype in ['numeric', 'na-numeric']:\n",
    "        type_str = \"str - 'Numeric'\"\n",
    "        answer_str = \"float - a single number (e.g., 92.5) or NA if the answer is not in the note\"\n",
    "        json_example = f\"\"\"```json\n",
    "{{\n",
    "    \"question\": \"What was the patient's highest creatinine measurement recorded in the note?\",\n",
    "    \"type\": \"Numeric\",\n",
    "    \"answer\": \"1.4\",\n",
    "}}```\"\"\"\n",
    "        # \"difficulty\": \"4\",\n",
    "    else:\n",
    "        print(qtype)\n",
    "        raise Exception('Not Implemented')\n",
    "\n",
    "    prompt = f\"\"\"***PATIENT NOTE:\n",
    "    {note}\n",
    "\n",
    "    Answer the following \n",
    "    *** QUESTION:\n",
    "    {question}\n",
    "\n",
    "    *** Format your response as a JSON object with the following keys: \n",
    "    * question: str - the question you were asked to answer, answer 'Not Available' if you cannot answer the question based on the patient's note. \n",
    "    * type: {type_str}\n",
    "    * answer: {answer_str}    \n",
    "\n",
    "    An example of how your JSON response should be formatted is shown below (this is only an example of one question, others should be in a list, your answer should be based on the ***Patient Note provided above):\n",
    "    ***EXAMPLE RESPONSE:\n",
    "    {json_example}\n",
    "\n",
    "    Provide a response that can be directly read by the Python JSON Parser library based on the ***PATIENT NOTE at the beginning of this message.\n",
    "    \"\"\"\n",
    "    # * difficulty: int - a score from 1-10 indicating how difficult this question is to answer based on the note\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# string compatible isnan\n",
    "def isNaN(num):\n",
    "    return num != num\n",
    "\n",
    "def format_output(r):\n",
    "    if isNaN(r['answer']):\n",
    "        answer = 'NA'\n",
    "    else:\n",
    "        answer = r['answer']\n",
    "    output_json = f\"\"\"```json\n",
    "{{\n",
    "    \"question\" : \"{r['question']}\",\n",
    "    \"type\": \"{r['question_type']}\",\n",
    "    \"answer\": \"{answer}\",\n",
    "    \"section\": \"{r['section']}\",\n",
    "    \"source\": \"{r['source']}\",\n",
    "    \"explanation\": \"{r['explanation']}\"\n",
    "}}```\"\"\"\n",
    "    return output_json\n",
    "\n",
    "\n",
    "examples = []\n",
    "for i, row in tqdm(new_df.iterrows(), total=new_df.shape[0]):\n",
    "    example_dict = {\n",
    "            'id': i,\n",
    "            'output': format_output(row),\n",
    "            'prompt': create_prompt(row['text'], row['question'], qtype=row['question_type'])\n",
    "        }\n",
    "\n",
    "    examples.append(example_dict)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "save_df = pd.DataFrame(examples)\n",
    "print(new_df.shape, save_df.shape)\n",
    "# Shuffle and split new_df\n",
    "new_train_df, new_test_df = train_test_split(new_df, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "# Ensure the same split and indices for save_df\n",
    "save_train_df = save_df.loc[new_train_df.index]\n",
    "save_test_df = save_df.loc[new_test_df.index]\n",
    "\n",
    "print(new_train_df.shape, new_test_df.shape)\n",
    "print(save_train_df.shape, save_test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save\n",
    "from datasets import Dataset\n",
    "\n",
    "folder_path = \"../data/synthetic-raw\"\n",
    "\n",
    "new_train_df.to_csv(f\"{folder_path}/train_{input_file}_{min_difficulty}-{min_count}-nosupport.csv\", index=False, compression='gzip')\n",
    "new_test_df.to_csv(f\"{folder_path}/test_{input_file}_{min_difficulty}-{min_count}-nosupport.csv\", index=False, compression='gzip')\n",
    "\n",
    "save_train_ds = Dataset.from_pandas(save_train_df)\n",
    "save_test_ds = Dataset.from_pandas(save_test_df)\n",
    "\n",
    "\n",
    "train_file = f\"{folder_path}/train_{input_file}_{min_difficulty}-{min_count}-nosupport.arrow\"\n",
    "test_file = f\"{folder_path}/test_{input_file}_{min_difficulty}-{min_count}-nosupport.arrow\"\n",
    "\n",
    "save_train_ds.save_to_disk(train_file)\n",
    "save_test_ds.save_to_disk(test_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
