{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "model_id_list = ['Llama-3.2-1B-Instruct', 'Llama-3.2-1B_all_1_8_16', 'Llama-3.2-3B-Instruct', 'Llama-3.2-3B_all_1_8_16'] \n",
    "\n",
    "# Function to load CSV files from a folder and add a 'question_type' column\n",
    "def load_csv_with_question_type(folder_path):\n",
    "    # Initialize an empty DataFrame to store all data\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    # Loop through all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file is a CSV file\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Get the question_type from the file name (text prior to the first underscore)\n",
    "            label_type = file_name\n",
    "\n",
    "            # Load the CSV file into a DataFrame\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['label_type'] = label_type\n",
    "            \n",
    "            # Append the data to the all_data DataFrame\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "    return all_data\n",
    "\n",
    "full_df = None\n",
    "for model_id in model_id_list:\n",
    "    model_id = model_id.split('/')[-1]\n",
    "    print(model_id)\n",
    "    \n",
    "    folder_path = f\"../outputs/i2b2/{model_id}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        continue \n",
    "    \n",
    "    print(folder_path)\n",
    "    df = load_csv_with_question_type(folder_path)\n",
    "    df['model'] = model_id\n",
    "\n",
    "    if full_df is None:\n",
    "        full_df = df\n",
    "    else:\n",
    "        full_df = pd.concat([full_df, df], axis=0)\n",
    "    \n",
    "    print(full_df.shape)\n",
    "\n",
    "# full_df = full_df[~full_df['question_label'].isna()]\n",
    "df = full_df\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def extract_answer_from_json(text, start_delimiter='```json', end_delimiter='```'):\n",
    "    try:\n",
    "        # start_index = text.index(start_delimiter) + len(start_delimiter)\n",
    "        # start_index=0\n",
    "        # end_index = text.index(end_delimiter, start_index)\n",
    "        # json_string = text[start_index:end_index].strip()\n",
    "        json_string = text\n",
    "\n",
    "        # Extract the answer value using regex\n",
    "        match = re.search(r'\"answer\"\\s*:\\s*(\".*?\"|\\d+(\\.\\d+)?)', json_string)\n",
    "        if match:\n",
    "            value = match.group(1)\n",
    "            # Remove quotes if the value is a string\n",
    "            if value.startswith('\"') and value.endswith('\"'):\n",
    "                value = value[1:-1]\n",
    "            return value\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: Error extracting answer in text: {text}\\n{e}\")\n",
    "        return None\n",
    "    \n",
    "def extract_info_from_json(df, json_column):\n",
    "    extracted_data = []\n",
    "    failure_list = []\n",
    "\n",
    "    regex_patterns = {\n",
    "        \"question\": r'\"question\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        \"type\": r'\"type\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        \"answer\": r'\"answer\"\\s*:\\s*\"([^\"]+)\"|:\\s*([\\d\\.]+)',\n",
    "        \"section\": r'\"section\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        \"source\": r'\"source\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        \"explanation\": r'\"explanation\"\\s*:\\s*\"([^\"]+)\"'\n",
    "    }\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        json_string = row[json_column]\n",
    "        extracted_row = {\"question\": None, \"type\": None, \"answer\": None, \"section\": None, \"source\": None, \"explanation\": None}\n",
    "        \n",
    "        # Handle leading/trailing unwanted characters\n",
    "        json_string = json_string.strip().replace('***', '').replace('```', '').replace('json', '')\n",
    "\n",
    "        # Handle cases where the string contains multiple JSON objects\n",
    "        json_objects = re.findall(r'\\{.*?\\}', json_string)\n",
    "        if json_objects:\n",
    "            json_string = json_objects[0]\n",
    "\n",
    "        # Try to parse the JSON string\n",
    "        try:\n",
    "            data = json.loads(json_string)\n",
    "            if isinstance(data, list):\n",
    "                data = data[0]  # Take the first object from a list\n",
    "\n",
    "            # Extract the required fields\n",
    "            for key in extracted_row:\n",
    "                extracted_row[key] = data.get(key)\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            # Fall back to regex if JSON parsing fails\n",
    "            for key, pattern in regex_patterns.items():\n",
    "                match = re.search(pattern, json_string)\n",
    "                if match:\n",
    "                    extracted_row[key] = match.group(1) or match.group(2)\n",
    "            # continue\n",
    "\n",
    "        extracted_row['model'] = row['model']\n",
    "        extracted_row['label'] = row['label']\n",
    "        extracted_row['question_label'] = row['question_label']\n",
    "        extracted_row['label_type'] = row['label_type'].split('_')[0]\n",
    "        extracted_row['format'] = row['label_type'].split('_')[1]\n",
    "\n",
    "        # Add even if we don't have the 'answer'\n",
    "        if extracted_row['answer'] is None:\n",
    "            failure_list.append(idx)\n",
    "            extracted_data.append(extracted_row)\n",
    "        else:\n",
    "            extracted_data.append(extracted_row)\n",
    "\n",
    "    extracted_df = pd.DataFrame(extracted_data)\n",
    "    return extracted_df, failure_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df, info_failure_list  = extract_info_from_json(df, 'output')\n",
    "print(info_df.shape, len(info_failure_list))\n",
    "info_df[info_df['label'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[info_failure_list]['output'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each row and extract the \"answer\" element\n",
    "def process_row(row):\n",
    "    row['answer'] = extract_answer_from_json(row['output'])\n",
    "    return row\n",
    "\n",
    "# Applying the function to each row\n",
    "df = df.apply(process_row, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['answer'].isna()].shape)\n",
    "print(df[df['answer'].isna()]['model'].value_counts())\n",
    "\n",
    "print(info_df[info_df['answer'].isna()].shape)\n",
    "print(info_df[info_df['answer'].isna()]['model'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('question_label')['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score, f1_score\n",
    "\n",
    "df.fillna('No', inplace=True)\n",
    "# Convert the 'answer' column to binary format\n",
    "df['answer_binary'] = df['answer'].apply(lambda x: 1 if x and x.lower() == 'yes' else 0)\n",
    "\n",
    "# Group by 'label_type' and 'model' and calculate metrics\n",
    "metrics = []\n",
    "\n",
    "for (question_label, label_type, model), group in df.groupby(['question_label', 'label_type', 'model']):\n",
    "    y_true = group['label']\n",
    "\n",
    "    # @TODO this would be better if including Sex (e.g., should be 1.1 for women, 1.3 for men)\n",
    "    if question_label == 'CREATININE_num':\n",
    "        y_pred_numeric = pd.to_numeric(group['answer'], errors='coerce')\n",
    "        y_pred_filled = y_pred_numeric.fillna(0)\n",
    "        y_pred = (y_pred_filled > 1.3).astype(int)\n",
    "    elif question_label == 'HBA1C_num':\n",
    "        y_pred_numeric = pd.to_numeric(group['answer'], errors='coerce')\n",
    "        y_pred_filled = y_pred_numeric.fillna(0)\n",
    "        y_pred = ((y_pred_filled >= 6.5) & ((y_pred_filled <= 9.5))).astype(int)\n",
    "        # y_pred = ((y_pred_filled >= 6.5)).astype(int)\n",
    "    else:\n",
    "        y_pred = group['answer_binary']\n",
    "    \n",
    "    auc = roc_auc_score(y_true, y_pred) if len(y_true.unique()) > 1 else None\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    metrics.append({\n",
    "        'train_test': label_type.split('_')[0],\n",
    "        'format': label_type.split('_')[1],\n",
    "        'question_label': question_label,\n",
    "        'model': model,\n",
    "        'auc': auc,\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1\n",
    "    })\n",
    "\n",
    "# Creating a DataFrame for metrics\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "display(metrics_df.sort_values(by='question_label'))\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.groupby(['model', 'train_test'])[['auc', 'accuracy', 'balanced_accuracy', 'macro_f1', 'micro_f1']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.groupby(['model', 'train_test', 'question_label'])[['accuracy', 'balanced_accuracy', 'macro_f1', 'micro_f1']].mean() #.to_csv('../results/eval/performance.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
