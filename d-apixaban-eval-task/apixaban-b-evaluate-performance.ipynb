{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to load CSV files from a folder and add a 'question_type' column\n",
    "def load_csv_with_question_type(folder_path):\n",
    "    # Initialize an empty DataFrame to store all data\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    # Loop through all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file is a CSV file\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Get the question_type from the file name (text prior to the first underscore)\n",
    "            label_type = file_name\n",
    "\n",
    "            # Load the CSV file into a DataFrame\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['label_type'] = label_type\n",
    "\n",
    "            # Append the data to the all_data DataFrame\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "    return all_data\n",
    "\n",
    "dataset = 'apixaban'\n",
    "# train_test = ['test', 'train']\n",
    "train_test = ['prompts']\n",
    "\n",
    "model_id_list = ['Llama-3.2-1B-Instruct', 'Llama-3.2-1B_all_1_8_16', 'Llama-3.2-3B-Instruct', 'Llama-3.2-3B_all_1_8_16'] \n",
    "\n",
    "# temperature_list = [0, 0.3, 0.5, 0.7, 0.8, 1]\n",
    "temperature_list = [0]\n",
    "\n",
    "param_folder_list = []\n",
    "for tt in train_test:\n",
    "    for model_id in model_id_list: \n",
    "        for temp in temperature_list:\n",
    "            if temp == 1:\n",
    "                top_p_list = [0.5, 0.7, 0.9]\n",
    "            else:\n",
    "                top_p_list = [1]\n",
    "                \n",
    "            for top_p in top_p_list:\n",
    "                param_folder_list.append(\n",
    "                    {\n",
    "                        'train_test': tt,\n",
    "                        'model_id': model_id,\n",
    "                        'temperature': temp,\n",
    "                        'top_p': top_p\n",
    "                    }\n",
    "                )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "full_df = None\n",
    "for params in param_folder_list:\n",
    "    # folder_path = f\"./outputs/apixaban-manuscript-run/params/{params['model_id']}/{params['temperature']}_{params['top_p']}\"\n",
    "    folder_path = f\"./outputs/apixaban/params/{params['model_id']}/{params['temperature']}_{params['top_p']}\"\n",
    "    print(folder_path)\n",
    "    \n",
    "    # Check if folder_path exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        continue  # Skip to the next iteration if the folder doesn't exist\n",
    "    \n",
    "    df = load_csv_with_question_type(folder_path)\n",
    "    df['model'] = params['model_id']\n",
    "    df['temperature'] = params['temperature']\n",
    "    df['top_p'] = params['top_p']\n",
    "    df['train_test'] = params['train_test']\n",
    "\n",
    "    if full_df is None:\n",
    "        full_df = df\n",
    "    else:\n",
    "        full_df = pd.concat([full_df, df], axis=0)\n",
    "    \n",
    "    print(full_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df.shape)\n",
    "full_df = full_df.rename(columns={'label': 'label_name'})\n",
    "full_df = full_df.rename(columns={'answer': 'label'})\n",
    "\n",
    "if full_df is not None:\n",
    "    full_df = full_df[~full_df['label_name'].isna()]\n",
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# full_df['label_name'] = full_df['label']\n",
    "# full_df['label'] = full_df['answer']\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_answer_from_json(text, start_delimiter='```json', end_delimiter='```'):\n",
    "    if not isinstance(text, str):\n",
    "        # If text is not a string, return None\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        start_index = text.index(start_delimiter) + len(start_delimiter)\n",
    "        end_index = text.index(end_delimiter, start_index)\n",
    "        json_string = text[start_index:end_index].strip()\n",
    "\n",
    "        # Extract the answer value using regex\n",
    "        match = re.search(r'\"answer\"\\s*:\\s*(\".*?\"|\\d+(\\.\\d+)?)', json_string)\n",
    "        if match:\n",
    "            value = match.group(1)\n",
    "            # Remove quotes if the value is a string\n",
    "            if value.startswith('\"') and value.endswith('\"'):\n",
    "                value = value[1:-1]\n",
    "            return value\n",
    "        return None\n",
    "    except (ValueError, AttributeError) as e:\n",
    "        # Handle ValueError and any unexpected AttributeError \n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_info_from_json(df, json_column):\n",
    "    extracted_data = []\n",
    "    failure_list = []\n",
    "\n",
    "    regex_patterns = {\n",
    "        \"question\": r'\"question\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        \"type\": r'\"type\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        \"answer\": r'\"answer\"\\s*:\\s*\"([^\"]+)\"|:\\s*([\\d\\.]+)',\n",
    "        \"section\": r'\"section\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        \"source\": r'\"source\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        \"explanation\": r'\"explanation\"\\s*:\\s*\"([^\"]+)\"'\n",
    "    }\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        json_string = row[json_column]\n",
    "        extracted_row = {\"question\": None, \"type\": None, \"answer\": None, \"section\": None, \"source\": None, \"explanation\": None}\n",
    "        \n",
    "\n",
    "        # if extracted_row['model'].isna() or extracted_row['label'].isna() or extracted_row['question_label'].isna():\n",
    "        #     print(row)\n",
    "        #     raise Exception('stop')\n",
    "        \n",
    "        # Handle leading/trailing unwanted characters\n",
    "        try:\n",
    "            json_string = json_string.strip().replace('***', '').replace('```', '').replace('json', '')\n",
    "        except:\n",
    "            failure_list.append(idx)\n",
    "            extracted_row['model'] = row['model']\n",
    "            extracted_row['label'] = row['label']\n",
    "            extracted_row['question_label'] = row['question_label']\n",
    "            extracted_row['label_type'] = row['label_type']\n",
    "            extracted_row['temperature'] = row['temperature']\n",
    "            extracted_row['top_p'] = row['top_p']\n",
    "           \n",
    "            extracted_data.append(extracted_row)\n",
    "            continue\n",
    "\n",
    "        # Handle cases where the string contains multiple JSON objects\n",
    "        json_objects = re.findall(r'\\{.*?\\}', json_string)\n",
    "        if json_objects:\n",
    "            json_string = json_objects[0]\n",
    "\n",
    "        # Try to parse the JSON string\n",
    "        try:\n",
    "            data = json.loads(json_string)\n",
    "            if isinstance(data, list):\n",
    "                data = data[0]  # Take the first object from a list\n",
    "\n",
    "            # Extract the required fields\n",
    "            for key in extracted_row:\n",
    "                extracted_row[key] = data.get(key)\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            # Fall back to regex if JSON parsing fails\n",
    "            for key, pattern in regex_patterns.items():\n",
    "                match = re.search(pattern, json_string)\n",
    "                if match:\n",
    "                    extracted_row[key] = match.group(1) or match.group(2)\n",
    "\n",
    "        extracted_row['model'] = row['model']\n",
    "        extracted_row['label'] = row['label']\n",
    "        extracted_row['question_label'] = row['question_label']\n",
    "        extracted_row['label_type'] = row['label_type']\n",
    "        extracted_row['temperature'] = row['temperature']\n",
    "        extracted_row['top_p'] = row['top_p']\n",
    "        \n",
    "\n",
    "        # Add even if we don't have the 'answer'\n",
    "        if extracted_row['answer'] is None:\n",
    "            failure_list.append(idx)\n",
    "            extracted_data.append(extracted_row)\n",
    "        else:\n",
    "            extracted_data.append(extracted_row)\n",
    "\n",
    "    extracted_df = pd.DataFrame(extracted_data)\n",
    "    return extracted_df, failure_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df, info_failure_list  = extract_info_from_json(full_df, 'output')\n",
    "print(info_df.shape, len(info_failure_list))\n",
    "# print('na label')\n",
    "# display(info_df[info_df['label'].isna()])\n",
    "display(info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "info_df[((info_df['question_label'] == 'PLT') &\n",
    "         (info_df['model'] == 'Meta-Llama-3.1-8B_all_1-25000-hardest'))][['answer', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "info_df[((info_df['question_label'] == 'hemorrhagic') &\n",
    "         (info_df['model'] == 'Meta-Llama-3.1-8B_all_1'))][['answer', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.loc[info_df['type'].isin(['yes', 'na-bool']), 'answer'].fillna('No', inplace=True)\n",
    "info_df.loc[info_df['type'].isin(['yes', 'na-bool']), 'label'].fillna('No', inplace=True)\n",
    "info_df.loc[info_df['type'].isin(['numeric', 'na-numeric']), 'answer'].fillna('NA', inplace=True)\n",
    "info_df.loc[info_df['type'].isin(['numeric', 'na-numeric']), 'label'].fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 400)\n",
    "# display(info_df.groupby('question_label')[['label']].value_counts())\n",
    "# pd.set_option('display.max_rows', 40)\n",
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 800)\n",
    "# display(info_df.groupby('question_label')[['answer']].value_counts())\n",
    "# pd.set_option('display.max_rows', 40)\n",
    "\n",
    "bool_question_definitions = {\n",
    "    'afib': 'Does the note describe the patient as having atrial fibrillation (afib)? Answer \"No\" if the note describes the patient as having afib secondary to another reversible cause?', \n",
    "    'mdd': 'Does the note describe the patient as ever being diagnosed with depression or major depressive disorder (MDD)? Answer \"No\" unless the note describes a diagnosis or history of depression.',\n",
    "    'schizophrenia': 'Does the note describe the patient as ever being diagnosed with schizophrenia or any schizoaffective disorders? Answer \"No\" unless the note describes a diagnosis or history of a schizoaffective disorder.',\n",
    "    'bipolar': 'Does the note describe the patient as ever being diagnosed with bipolar disorder?  Answer \"No\" unless the note describes a diagnosis or history of bipolar disorder', \n",
    "    'hemorrhagic': 'Does the note describe the patient as ever having any hemorrhagic tendencies or blood dyscrasias (i.e., any disorder of the blood, bone marrow, clotting proteins, or lymph tissue)? Examples could include anemia (hemoglobin deficiency), leukopenia (low WBC count), thrompocytopenia (low platelent count),  any forms of leukemia, or clotting disorders.  Answer \"No\" unless the note describes a diagnosis or history that could be considered a blood or clotting disorder.',\n",
    "    'recent_stroke': 'Does the note describe the patient as having a stroke during this admission or within the last month? (Answer \"Yes\" for a stroke within the last 30 days or if it was recent but the date is unclear, answer \"No\" if no stroke is mentioned or a prior stroke occurred but was not recent)',\n",
    "    'peptic_ulcer_disease': 'Does the note describe the patient as ever having peptic ulcer disease?',\n",
    "    'bleeding': 'Does the note describe the patient as having serious bleeding (e.g., hemorrhage) in the past 6 months? Answer \"No\" unless the note describes a serious recent bleeding issue.',\n",
    "    'afib_ablation': 'Does the note describe the patient as having a planned or past ablation procedure for afib? Answer \"No\" unless the note includes information about a past or planned ablation for afib.',\n",
    "    'surgical_valvular_disease': 'Does the note describe the patient as ever having valvular disease (stenosis) requiring surgery? Answer \"No\" if there is mention of stenosis without surgery.',\n",
    "    'heart_failure': 'Does the note describe the patient as having heart failure?',\n",
    "    't2d': 'Does the note describe the patient as ever having type 2 diabetes (T2D)? Answer \"No\" if the note does not include a diagnosis or history of Type 2 diabetes, T2D, Type II diabetes etc..',\n",
    "    't2d-1': 'Does the note describe the patient as having diabetes mellitus (DM1, DM2, T2D, T1DM, T2DM)?',\n",
    "    'arterial_hypertension': 'Does the note describe the patient as having arterial hypertension (high bp e.g. >140, or HTN)? This includes pre-existing hypertension and treated hypertension.',\n",
    "    'prior_stroke': 'Does the note describe the patient as ever having a stroke or transient ischemic attack (TIA)? Answer \"No\" unless the note includes information about the patient having a prior stroke or TIA',\n",
    "    'med_decisions': 'Does the note describe the patient as being unable to make medical decisions upon discharge? Answer \"No\" unless there is evidence the patient cannot make their own medical decisions because they are not cabable (for example if they are not mentally competent, not awake, not conscious, have dementia, or are deceased) or there is evidence someone else (e.g., a husband, wife, family member, or attorney) is designated to make their medical decisions.'\n",
    "}\n",
    "\n",
    "num_question_definitions = {\n",
    "    'PLT': 'What is the lowest platelet count (PLT) recorded for the patient in the note? Answer \"NA\" if there is no platelet count (PLT) that can be found in the note.', \n",
    "    'BILI': 'What is the higest total bilirubin (TotBili, Bili) mentioned in the note? Answer \"NA\" unless there is a numeric bilirubin (or total bilirubin) count in the note.', \n",
    "    'AST': 'What is the higest aspartate aminotransferase level (AST) mentioned in the note? Answer \"NA\" if no AST value is available in the note.',\n",
    "    'CREAT': 'What is the higest serum creatinine (Creat) mentioned in the note? Answer \"NA\" if no creatinine value is available in the note.',\n",
    "    'HGB': 'What is the lowest hemoglobin (HGB) mentioned in the note? Answer \"NA\" if no HGB value is available in the note.',\n",
    "    'chads2': 'What is the highest CHADS2 score mentioned? Answer \"NA\" if no CHADS2 score is in the note.',\n",
    "    'lvef': 'What is the lowest left ventricular ejection (LVEF, ef, ejection fraction) fraction mentioned in the note? Answer \"NA\" if no LVEF is in the note, Answer 55 if the lowest value is 55%% or greater.',\n",
    "    'blood_glucose': 'What is the highest blood glucose lab mentioned? Answer \"NA\" if no blood glucose score is in the note.',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def preprocess_values(y_true, y_pred):\n",
    "    def normalize(val):\n",
    "        if isinstance(val, str):\n",
    "            return val.strip().lower()\n",
    "        elif isinstance(val, (int, float)):\n",
    "            return float(val)\n",
    "        else:\n",
    "            return val\n",
    "    \n",
    "    y_true_normalized = [normalize(val) for val in y_true]\n",
    "    y_pred_normalized = [normalize(val) for val in y_pred]\n",
    "    \n",
    "    return np.array(y_true_normalized), np.array(y_pred_normalized)\n",
    "\n",
    "metrics = []\n",
    "\n",
    "info_df['label'].fillna('NA', inplace=True)\n",
    "info_df['answer'].fillna('NA', inplace=True)\n",
    "\n",
    "def convert_to_binary(y_true, y_pred):\n",
    "    return np.array([1 if yt == yp else 0 for yt, yp in zip(y_true, y_pred)])\n",
    "\n",
    "# Example: iterate over the groups\n",
    "for (question_label, type, model, temperature, top_p), group in info_df.groupby(['question_label', 'type', 'model', 'temperature', 'top_p']):\n",
    "    if question_label in bool_question_definitions.keys():\n",
    "        y_pred = group['label'].apply(lambda x: 1 if x and x.lower() == 'yes' else 0)\n",
    "        y_true = group['answer'].apply(lambda x: 1 if x and x.lower() == 'yes' else 0)    \n",
    "\n",
    "        auc = roc_auc_score(y_true, y_pred) if len(y_true.unique()) > 1 else 1\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "        macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "    elif question_label in num_question_definitions.keys():\n",
    "        y_true = group['label']\n",
    "        y_pred = group['answer']\n",
    "        y_true, y_pred = preprocess_values(y_true, y_pred)\n",
    "        y_true_binary = convert_to_binary(y_true, y_true)\n",
    "        y_pred_binary = convert_to_binary(y_true, y_pred)\n",
    "        accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "        balanced_acc = balanced_accuracy_score(y_true_binary, y_pred_binary)\n",
    "        macro_f1 = f1_score(y_true_binary, y_pred_binary, average='macro')\n",
    "        micro_f1 = f1_score(y_true_binary, y_pred_binary, average='micro')\n",
    "    else:\n",
    "        continue\n",
    "        # raise Exception('Not Implemented')\n",
    "\n",
    "    metrics.append({\n",
    "        'question_label': question_label,\n",
    "        'model': model,\n",
    "        'temperature': temperature,\n",
    "        'top_p': top_p,\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1\n",
    "    })\n",
    "\n",
    "metrics_info_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Create the pivot table\n",
    "pivot_table = metrics_info_df.pivot_table(\n",
    "    index='question_label',\n",
    "    columns='model',\n",
    "    values=['balanced_accuracy', 'micro_f1'],\n",
    "    aggfunc='mean'\n",
    ")\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def preprocess_values(y_true, y_pred):\n",
    "    def normalize(val):\n",
    "        if isinstance(val, str):\n",
    "            return val.strip().lower()\n",
    "        elif isinstance(val, (int, float)):\n",
    "            return float(val)\n",
    "        else:\n",
    "            return val\n",
    "    \n",
    "    y_true_normalized = [normalize(val) for val in y_true]\n",
    "    y_pred_normalized = [normalize(val) for val in y_pred]\n",
    "    \n",
    "    return np.array(y_true_normalized), np.array(y_pred_normalized)\n",
    "\n",
    "metrics = []\n",
    "\n",
    "info_df['label'].fillna('NA', inplace=True)\n",
    "info_df['answer'].fillna('NA', inplace=True)\n",
    "\n",
    "def convert_to_binary(y_true, y_pred):\n",
    "    return np.array([1 if yt == yp else 0 for yt, yp in zip(y_true, y_pred)])\n",
    "\n",
    "# Example: iterate over the groups\n",
    "for (question_label, type, model, temperature, top_p), group in info_df.groupby(['question_label', 'type', 'model', 'temperature', 'top_p']):\n",
    "    if question_label in bool_question_definitions.keys():\n",
    "        y_pred = group['label'].apply(lambda x: 1 if x and x.lower() == 'yes' else 0)\n",
    "        y_true = group['answer'].apply(lambda x: 1 if x and x.lower() == 'yes' else 0)    \n",
    "\n",
    "        auc = roc_auc_score(y_true, y_pred) if len(y_true.unique()) > 1 else 1\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "        macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "    elif question_label in num_question_definitions.keys():\n",
    "        y_true = group['label']\n",
    "        y_pred = group['answer']\n",
    "        y_true, y_pred = preprocess_values(y_true, y_pred)\n",
    "        y_true_binary = convert_to_binary(y_true, y_true)\n",
    "        y_pred_binary = convert_to_binary(y_true, y_pred)\n",
    "        accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "        balanced_acc = balanced_accuracy_score(y_true_binary, y_pred_binary)\n",
    "        macro_f1 = f1_score(y_true_binary, y_pred_binary, average='macro')\n",
    "        micro_f1 = f1_score(y_true_binary, y_pred_binary, average='micro')\n",
    "    else:\n",
    "        continue\n",
    "        # raise Exception('Not Implemented')\n",
    "\n",
    "    metrics.append({\n",
    "        'question_label': question_label,\n",
    "        'model': model,\n",
    "        'temperature': temperature,\n",
    "        'top_p': top_p,\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1\n",
    "    })\n",
    "\n",
    "metrics_info_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Create the pivot table\n",
    "pivot_table = metrics_info_df.pivot_table(\n",
    "    index='question_label',\n",
    "    columns='model',\n",
    "    values=['balanced_accuracy', 'micro_f1'],\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Reindex columns for a specific order\n",
    "column_order = [\n",
    "    # ('balanced_accuracy', 'Meta-Llama-3.1-70B-Instruct'), \n",
    "    # ('balanced_accuracy', 'Meta-Llama-3.1-8B-Instruct'),\n",
    "    # ('balanced_accuracy', 'Meta-Llama-3.1-8B_all_1'), \n",
    "    # ('balanced_accuracy', 'Meta-Llama-3.1-8B_all_1-25000-hardest'),\n",
    "    # ('micro_f1', 'Meta-Llama-3.1-70B-Instruct'), \n",
    "    # ('micro_f1', 'Meta-Llama-3.1-8B-Instruct'),\n",
    "    # ('micro_f1', 'Meta-Llama-3.1-8B_all_1'), \n",
    "    # ('micro_f1', 'Meta-Llama-3.1-8B_all_1-25000-hardest')\n",
    "    ('balanced_accuracy', 'Llama-3.2-3B-Instruct'), \n",
    "    ('balanced_accuracy', 'Llama-3.2-3B_all_1_8_16'),\n",
    "    ('balanced_accuracy', 'Llama-3.2-1B-Instruct'), \n",
    "    ('balanced_accuracy', 'Llama-3.2-1B_all_1_8_16'),\n",
    "    ('micro_f1', 'Llama-3.2-3B-Instruct'), \n",
    "    ('micro_f1', 'Llama-3.2-3B_all_1_8_16'),\n",
    "    ('micro_f1', 'Llama-3.2-1B-Instruct'), \n",
    "    ('micro_f1', 'Llama-3.2-1B_all_1_8_16')\n",
    "    \n",
    "]\n",
    "pivot_table = pivot_table.reindex(columns=column_order)\n",
    "\n",
    "# Perform bootstrapping to calculate 95% CI for each model and metric\n",
    "def bootstrap_model_metric(df, metric, n=1000):\n",
    "    \"\"\"Bootstrap the average of a given metric and calculate the 95% CI for a model.\"\"\"\n",
    "    bootstrapped_means = []\n",
    "    \n",
    "    # Bootstrapping\n",
    "    for _ in range(n):\n",
    "        resampled_df = resample(df)\n",
    "        model_avg = resampled_df[metric].mean()\n",
    "        bootstrapped_means.append(model_avg)\n",
    "    \n",
    "    mean_avg = np.mean(bootstrapped_means) * 100  # Convert to percentage\n",
    "    lower_ci = np.percentile(bootstrapped_means, 2.5) * 100\n",
    "    upper_ci = np.percentile(bootstrapped_means, 97.5) * 100\n",
    "    return mean_avg, lower_ci, upper_ci\n",
    "\n",
    "# Calculate bootstrapped 95% CI for each model and metric (balanced_accuracy and micro_f1)\n",
    "metrics_with_ci = {}\n",
    "for metric in ['balanced_accuracy', 'micro_f1']:\n",
    "    for model in metrics_info_df['model'].unique():\n",
    "        model_df = metrics_info_df[metrics_info_df['model'] == model]\n",
    "        mean_avg, lower_ci, upper_ci = bootstrap_model_metric(model_df, metric)\n",
    "        metrics_with_ci[(metric, model)] = f\"{mean_avg:.1f}% ({lower_ci:.1f}%, {upper_ci:.1f}%)\"\n",
    "\n",
    "# Formatting the table and adding bootstrapped averages with CI for each model\n",
    "formatted_pivot_table = pivot_table.applymap(lambda x: f\"{x*100:.2f}%\")\n",
    "for (metric, model), ci_value in metrics_with_ci.items():\n",
    "    formatted_pivot_table.loc['Average', (metric, model)] = ci_value\n",
    "\n",
    "# Convert the pivot table to CSV format for Excel copy-pasting\n",
    "csv_output = formatted_pivot_table.to_csv()\n",
    "\n",
    "# Print CSV output\n",
    "print(csv_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score, f1_score\n",
    "\n",
    "def preprocess_values(y_true, y_pred):\n",
    "    # Define a helper function to handle numeric and string comparisons\n",
    "    def normalize(val):\n",
    "        if isinstance(val, str):\n",
    "            # If string, strip spaces and lower the case\n",
    "            return val.strip().lower()\n",
    "        elif isinstance(val, (int, float)):\n",
    "            # If numeric, convert to float for comparison (handles 188 == 188.0)\n",
    "            return float(val)\n",
    "        else:\n",
    "            return val\n",
    "    \n",
    "    y_true_normalized = [normalize(val) for val in y_true]\n",
    "    y_pred_normalized = [normalize(val) for val in y_pred]\n",
    "    \n",
    "    return np.array(y_true_normalized), np.array(y_pred_normalized)\n",
    "\n",
    "metrics = []\n",
    "\n",
    "info_df['label'].fillna('NA', inplace=True)\n",
    "info_df['answer'].fillna('NA', inplace=True)\n",
    "\n",
    "def convert_to_binary(y_true, y_pred):\n",
    "    return np.array([1 if yt == yp else 0 for yt, yp in zip(y_true, y_pred)])\n",
    "\n",
    "# Example: iterate over the groups\n",
    "for (question_label, type, model, temperature, top_p), group in info_df.groupby(['question_label', 'type', 'model', 'temperature', 'top_p']):\n",
    "    print(question_label)\n",
    "    if question_label in bool_question_definitions.keys():\n",
    "        print('Boolean')\n",
    "        y_pred = group['label'].apply(lambda x: 1 if x and x.lower() == 'yes' else 0)\n",
    "        y_true = group['answer'].apply(lambda x: 1 if x and x.lower() == 'yes' else 0)    \n",
    "\n",
    "        auc = roc_auc_score(y_true, y_pred) if len(y_true.unique()) > 1 else 1\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "        macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "    elif question_label in num_question_definitions.keys():\n",
    "        print('Numeric')\n",
    "        y_true = group['label']\n",
    "        y_pred = group['answer']\n",
    "\n",
    "        # Preprocess y_true and y_pred\n",
    "        y_true, y_pred = preprocess_values(y_true, y_pred)\n",
    "        y_true_binary = convert_to_binary(y_true, y_true)  # Ground truth stays as is\n",
    "        y_pred_binary = convert_to_binary(y_true, y_pred)  # Prediction binary matches with true values\n",
    "\n",
    "        # Compute statistics\n",
    "        # auc = roc_auc_score(y_true, y_pred) if len(np.unique(y_true)) > 1 else 1\n",
    "        accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "        balanced_acc = balanced_accuracy_score(y_true_binary, y_pred_binary)\n",
    "        macro_f1 = f1_score(y_true_binary, y_pred_binary, average='macro')\n",
    "        micro_f1 = f1_score(y_true_binary, y_pred_binary, average='micro')\n",
    "    else:\n",
    "        continue\n",
    "        # raise Exception('Not Implemented')\n",
    "\n",
    "    \n",
    "    \n",
    "    # Print or store the statistics as needed\n",
    "    metrics.append({\n",
    "        'question_label': question_label,\n",
    "        'model': model,\n",
    "        'temperature': temperature,\n",
    "        'top_p': top_p,\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1\n",
    "    })\n",
    "\n",
    "metrics_info_df = pd.DataFrame(metrics)\n",
    "# pd.set_option('display.max_rows', 400)\n",
    "# display(metrics_info_df.sort_values(by='question_label'))\n",
    "# pd.set_option('display.max_rows', 20)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "display(metrics_info_df[metrics_info_df['model']=='Meta-Llama-3.1-8B_all_1-25000-hardest'].groupby('question_label')['accuracy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_info_df['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metrics_info_df[metrics_info_df['model']=='Meta-Llama-3.1-8B-Instruct'].groupby('question_label')['accuracy'].mean()-metrics_info_df[metrics_info_df['model']=='Meta-Llama-3.1-8B-Instruct'].groupby('question_label')['accuracy'].mean())\n",
    "# display(metrics_info_df[metrics_info_df['model']=='Meta-Llama-3.1-8B_all_1'].groupby('question_label')['accuracy'].mean()-metrics_info_df[metrics_info_df['model']=='Meta-Llama-3.1-8B-Instruct'].groupby('question_label')['accuracy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 400)\n",
    "display(metrics_info_df[['question_label', 'model', 'balanced_accuracy']])\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_info_df = metrics_info_df[metrics_info_df['question_label'] != 'PLT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "# Assuming your data is already loaded into metrics_info_df\n",
    "# Pivot table creation\n",
    "pivot_table = metrics_info_df.pivot_table(\n",
    "    index='question_label',\n",
    "    columns='model',\n",
    "    values=['balanced_accuracy', 'micro_f1'],\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Specifying column order (replace placeholders with your actual model names)\n",
    "column_order = [\n",
    "    ('balanced_accuracy', 'Meta-Llama-3.1-70B-Instruct'), \n",
    "    ('balanced_accuracy', 'Meta-Llama-3.1-8B-Instruct'),\n",
    "    ('balanced_accuracy', 'Meta-Llama-3.1-8B_all_1'), \n",
    "    ('balanced_accuracy', 'Meta-Llama-3.1-8B_all_1-25000-hardest'),\n",
    "    ('micro_f1', 'Meta-Llama-3.1-70B-Instruct'), \n",
    "    ('micro_f1', 'Meta-Llama-3.1-8B-Instruct'),\n",
    "    ('micro_f1', 'Meta-Llama-3.1-8B_all_1'), \n",
    "    ('micro_f1', 'Meta-Llama-3.1-8B_all_1-25000-hardest')\n",
    "]\n",
    "pivot_table = pivot_table.reindex(columns=column_order)\n",
    "\n",
    "# Formatting to two decimal places for all values including the average\n",
    "formatted_pivot_table = pivot_table.applymap(lambda x: f\"{x:.2f}\")\n",
    "formatted_pivot_table.loc['Average'] = pivot_table.mean().apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "# Displaying the formatted table with averages formatted\n",
    "formatted_pivot_table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
